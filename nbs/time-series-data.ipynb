{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3d99f4",
   "metadata": {},
   "source": [
    "## Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59a89b",
   "metadata": {},
   "source": [
    "Create a Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_date_range = pd.date_range(start='1/1/2018', end='1/08/2018', freq='H')\n",
    "a_date_range\n",
    "\n",
    "# DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',\n",
    "#                '2018-01-01 02:00:00', '2018-01-01 03:00:00',\n",
    "#                '2018-01-01 04:00:00', '2018-01-01 05:00:00',\n",
    "#                '2018-01-01 06:00:00', '2018-01-01 07:00:00',\n",
    "#                '2018-01-01 08:00:00', '2018-01-01 09:00:00',\n",
    "#                ...\n",
    "#                '2018-01-07 15:00:00', '2018-01-07 16:00:00',\n",
    "#                '2018-01-07 17:00:00', '2018-01-07 18:00:00',\n",
    "#                '2018-01-07 19:00:00', '2018-01-07 20:00:00',\n",
    "#                '2018-01-07 21:00:00', '2018-01-07 22:00:00',\n",
    "#                '2018-01-07 23:00:00', '2018-01-08 00:00:00'],\n",
    "#               dtype='datetime64[ns]', length=169, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a_date_range[0])\n",
    "# pandas._libs.tslibs.timestamps.Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb933d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a_date_range, columns=['date'])\n",
    "df['data'] = np.random.randint(0,100, size=(len(a_date_range)))\n",
    "\n",
    "print(df.head(15))\n",
    "\n",
    "\n",
    "#                   date  data\n",
    "# 0  2018-01-01 00:00:00    99\n",
    "# 1  2018-01-01 01:00:00    15\n",
    "# 2  2018-01-01 02:00:00    79\n",
    "# 3  2018-01-01 03:00:00    37\n",
    "# 4  2018-01-01 04:00:00    27\n",
    "# 5  2018-01-01 05:00:00    51\n",
    "# 6  2018-01-01 06:00:00    20\n",
    "# 7  2018-01-01 07:00:00    87\n",
    "# 8  2018-01-01 08:00:00    86\n",
    "# 9  2018-01-01 09:00:00    31\n",
    "# 10 2018-01-01 10:00:00    29\n",
    "# 11 2018-01-01 11:00:00    74\n",
    "# 12 2018-01-01 12:00:00     8\n",
    "# 13 2018-01-01 13:00:00    95\n",
    "# 14 2018-01-01 14:00:00    42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('datetime')\n",
    "df.drop(['date'], axis=1, inplace=True) # remove all columns (axis=1) under column label 'date'\n",
    "print(df.head())\n",
    "\n",
    "#                     data\n",
    "# datetime                 \n",
    "# 2018-01-01 00:00:00    45\n",
    "# 2018-01-01 01:00:00    18\n",
    "# 2018-01-01 02:00:00    34\n",
    "# 2018-01-01 03:00:00     4\n",
    "# 2018-01-01 04:00:00    39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8084c",
   "metadata": {},
   "source": [
    "Convert string to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_date_range = [str(s) for s in a_date_range]\n",
    "\n",
    "timestamp_date_range = pd.to_datetime(string_date_range, infer_datetime_format=True)\n",
    "print(timestamp_date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eeb00c",
   "metadata": {},
   "source": [
    "Generally you want to be more explicit on your datetime conversion to prevent inference from being in consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_date_range_two = ['June-01-2018', 'June-02-2018', 'June-02-2018', 'June-03-2018']\n",
    "timestamp_date_range_two = [datetime.strptime(x, '%B-%d-%Y') for x in string_date_range_two]\n",
    "\n",
    "# [datetime.datetime(2018, 6, 1, 0, 0),\n",
    "#  datetime.datetime(2018, 6, 2, 0, 0),\n",
    "#  datetime.datetime(2018, 6, 2, 0, 0),\n",
    "#  datetime.datetime(2018, 6, 3, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.resample('D').mean())\n",
    "\n",
    "#                  data\n",
    "# datetime             \n",
    "# 2018-01-01  49.000000\n",
    "# 2018-01-02  42.333333\n",
    "# 2018-01-03  54.875000\n",
    "# 2018-01-04  50.083333\n",
    "# 2018-01-05  48.666667\n",
    "# 2018-01-06  46.583333\n",
    "# 2018-01-07  48.208333\n",
    "# 2018-01-08  27.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be016472",
   "metadata": {},
   "source": [
    "Is this is the oppposite of `pd.asfreq` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c56c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_sum'] = df.rolling(3).sum()\n",
    "print(df.head(10))\n",
    "\n",
    "#                      data  rolling_sum\n",
    "# datetime                              \n",
    "# 2018-01-01 00:00:00    45          NaN\n",
    "# 2018-01-01 01:00:00    18          NaN\n",
    "# 2018-01-01 02:00:00    34         97.0\n",
    "# 2018-01-01 03:00:00     4         56.0\n",
    "# 2018-01-01 04:00:00    39         77.0\n",
    "# 2018-01-01 05:00:00    60        103.0\n",
    "# 2018-01-01 06:00:00    26        125.0\n",
    "# 2018-01-01 07:00:00    80        166.0\n",
    "# 2018-01-01 08:00:00    47        153.0\n",
    "# 2018-01-01 09:00:00    86        213.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569327d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_sum'].fillna(method='backfill')\n",
    "\n",
    "# datetime\n",
    "# 2018-01-01 00:00:00     97.0\n",
    "# 2018-01-01 01:00:00     97.0\n",
    "# 2018-01-01 02:00:00     97.0\n",
    "# 2018-01-01 03:00:00     56.0\n",
    "# 2018-01-01 04:00:00     77.0\n",
    "#                        ...  \n",
    "# 2018-01-07 20:00:00    122.0\n",
    "# 2018-01-07 21:00:00    175.0\n",
    "# 2018-01-07 22:00:00    109.0\n",
    "# 2018-01-07 23:00:00    165.0\n",
    "# 2018-01-08 00:00:00    131.0\n",
    "# Name: rolling_sum, Length: 169, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c46eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_t = 1529272655\n",
    "real_t = pd.to_datetime(epoch_t, unit='s')\n",
    "real_t\n",
    "\n",
    "# Timestamp('2018-06-17 21:57:35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_t.tz_localize('UTC')  # set timezone attribute to UTC\n",
    "# Timestamp('2018-06-17 21:57:35+0000', tz='UTC')\n",
    "\n",
    "real_t.tz_localize('UTC').tz_convert('US/Pacific') # convert timezone from current attribute to another\n",
    "# Timestamp('2018-06-17 14:57:35-0700', tz='US/Pacific')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8dfbdc",
   "metadata": {},
   "source": [
    "Points to keep in mind during the preprocessing:\n",
    "1. Check for datetime format discrepencies: sudden changes from British to American datetime format are the worst kinds. So are things like changes to the timezone or daylight savings.\n",
    "2. Keep track of timezones: particularly when the time is reported in relation to local time which is typical of international modes of travel.\n",
    "3. Missing data, be able to justify by forward / backward fill is appropriate or any other decision.\n",
    "4. Keep track of each transformation as functions that can be documented, called upon and applied.\n",
    "5. Justify any resampling carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a7b3b",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "1. [Basic Time Series Manipulation with Pandas](https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea)\n",
    "2. [Time Series Analysis with Pandas - Manipulation and Plotting of Time Series in Python using Pandas Methods](https://ourcodingclub.github.io/tutorials/pandas-time-series/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md:myst",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
